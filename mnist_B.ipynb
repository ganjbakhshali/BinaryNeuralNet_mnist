{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z1iTBw_cT8XO"
      },
      "outputs": [],
      "source": [
        "## custom design Network Binary 1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Function\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Binarization function\n",
        "class BinaryActivation(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return input.sign()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.clamp_(-1, 1)\n",
        "\n",
        "def binary_activation(input):\n",
        "    return BinaryActivation.apply(input)\n",
        "\n",
        "# Define your binary neural network\n",
        "class BinaryNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = binary_activation(self.fc1(x))\n",
        "        x = binary_activation(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Train block\n",
        "\n",
        "# Initialize network\n",
        "model = BinaryNet().to(device)\n",
        "\n",
        "# Load MNIST\n",
        "mnist_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=mnist_transform, download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 11):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "print(\"Training Complete\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'binary_mnist_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6kqOy7nIGUt",
        "outputId": "a4fe190e-7bc6-416e-a4cf-e2537055fb99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 82662391.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 83772955.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 30976433.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16609005.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.2997497618198395\n",
            "Epoch 2, Loss: 0.29428812861442566\n",
            "Epoch 3, Loss: 0.3297218978404999\n",
            "Epoch 4, Loss: 0.19530165195465088\n",
            "Epoch 5, Loss: 0.056826308369636536\n",
            "Epoch 6, Loss: 0.43031495809555054\n",
            "Epoch 7, Loss: 0.36292341351509094\n",
            "Epoch 8, Loss: 0.16632799804210663\n",
            "Epoch 9, Loss: 0.5014564990997314\n",
            "Epoch 10, Loss: 0.391250342130661\n",
            "Training Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Test model before pruning\n",
        "\n",
        "# Load the saved model\n",
        "model = BinaryNet().to(device)\n",
        "model.load_state_dict(torch.load('binary_mnist_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Load the test dataset\n",
        "mnist_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=mnist_transform, download=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy (Model Before pruning): {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2oNvVSlH6Vj",
        "outputId": "22cdcfda-9668-4064-9781-906ef66233ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (Model Before pruning): 92.69%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "\n",
        "class PruningGeneticAlgorithm:\n",
        "    def __init__(self, model, dataset, population_size=50, generations=10, mutation_rate=0.01, crossover_rate=0.5):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.population_size = population_size\n",
        "        self.generations = generations\n",
        "        self.mutation_rate = mutation_rate\n",
        "        self.crossover_rate = crossover_rate\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def initial_population(self):\n",
        "        population = []\n",
        "        for _ in range(self.population_size):\n",
        "            individual = {name: torch.randint(0, 2, weight.shape).bool().to(self.device)\n",
        "                          for name, weight in self.model.named_parameters()}\n",
        "            population.append(individual)\n",
        "        return population\n",
        "\n",
        "    def fitness(self, individual):\n",
        "        # Apply the mask to the model\n",
        "        pruned_model = copy.deepcopy(self.model)\n",
        "        with torch.no_grad():\n",
        "            for name, param in pruned_model.named_parameters():\n",
        "                mask = individual.get(name, torch.ones_like(param.data).bool().to(self.device))\n",
        "                param.data.mul_(mask)\n",
        "        # Evaluate fitness with evaluation function -> accuracy\n",
        "        accuracy = self.evaluate(pruned_model)\n",
        "        return accuracy\n",
        "\n",
        "    def evaluate(self, model):\n",
        "        correct, total = 0, 0\n",
        "        dataloader = DataLoader(self.dataset, batch_size=64, shuffle=False)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "    def selection(self, population, fitnesses):\n",
        "        sorted_population = [x for _, x in sorted(zip(fitnesses, population), key=lambda pair: pair[0], reverse=True)]\n",
        "        return sorted_population[:int(0.2 * len(sorted_population))]\n",
        "\n",
        "    def crossover(self, parent1, parent2):\n",
        "        child = {}\n",
        "        for key in parent1.keys():\n",
        "            mask = torch.rand(parent1[key].size()).to(self.device) < self.crossover_rate\n",
        "            child[key] = mask.bool() * parent1[key] + (~mask).bool() * parent2[key]\n",
        "        return child\n",
        "\n",
        "    def mutation(self, individual):\n",
        "        for param in individual.values():\n",
        "            mutation_mask = torch.rand(param.size()).to(self.device) < self.mutation_rate\n",
        "            param.logical_xor_(mutation_mask)\n",
        "        return individual\n",
        "\n",
        "    def run(self):\n",
        "        population = self.initial_population()\n",
        "        for generation in range(self.generations):\n",
        "            fitnesses = [self.fitness(individual) for individual in population]\n",
        "            selected = self.selection(population, fitnesses)\n",
        "            children = []\n",
        "            while len(children) < self.population_size:\n",
        "                parent1, parent2 = np.random.choice(selected, size=2, replace=False)\n",
        "                child = self.crossover(parent1, parent2)\n",
        "                child = self.mutation(child)\n",
        "                children.append(child)\n",
        "            population = children\n",
        "\n",
        "            best_fitness = max(fitnesses)\n",
        "            print(f\"Generation {generation} -- Best Fitness: {best_fitness}\")\n",
        "\n",
        "        best_individual = population[np.argmax(fitnesses)]\n",
        "        return best_individual\n",
        "\n"
      ],
      "metadata": {
        "id": "x1gKL-qHXDDY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def test_pruned_model(model, validation_dataset, best_pruned_mask):\n",
        "#     # Apply the best mask to the model to get the pruned model\n",
        "#     pruned_model = copy.deepcopy(model)\n",
        "#     for name, param in pruned_model.named_parameters():\n",
        "#         mask = best_pruned_mask.get(name, torch.ones_like(param.data).bool())\n",
        "#         param.data.mul_(mask)\n",
        "\n",
        "#     # Evaluate the pruned model on the validation dataset\n",
        "#     accuracy = evaluate(pruned_model, validation_dataset)\n",
        "#     print(f\"Accuracy of the pruned model: {accuracy}\")\n",
        "\n",
        "def test_pruned_model(model, validation_dataset, best_pruned_mask):\n",
        "    # Apply the best mask to the model to get the pruned model\n",
        "    pruned_model = copy.deepcopy(model)\n",
        "    total_params = 0\n",
        "    pruned_params = 0\n",
        "\n",
        "    for name, param in pruned_model.named_parameters():\n",
        "        mask = best_pruned_mask.get(name, torch.ones_like(param.data).bool())\n",
        "        pruned_params += torch.sum(~mask).item()\n",
        "        total_params += torch.numel(param.data)\n",
        "\n",
        "        param.data.mul_(mask)\n",
        "\n",
        "    # Evaluate the pruned model on the validation dataset\n",
        "    accuracy = evaluate(pruned_model, validation_dataset)\n",
        "    print(f\"Accuracy of the pruned model: {accuracy}\")\n",
        "\n",
        "    # Calculate and print the percentage of weights pruned\n",
        "    percentage_pruned = (pruned_params / total_params) * 100\n",
        "    print(f\"Percentage of weights pruned: {percentage_pruned:.2f}%\")\n",
        "\n",
        "\n",
        "def evaluate(model, dataset):\n",
        "    correct, total = 0, 0\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    model = BinaryNet()\n",
        "    model.load_state_dict(torch.load('binary_mnist_model.pth'))\n",
        "\n",
        "\n",
        "    validation_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "    ga = PruningGeneticAlgorithm(model=model, dataset=validation_dataset)\n",
        "    best_pruned_mask = ga.run()\n",
        "\n",
        "    # Test the pruned model\n",
        "    test_pruned_model(model, validation_dataset, best_pruned_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oehRSOdTiBNN",
        "outputId": "d21642f8-36eb-4b0d-c5b5-f4e64edb5fa3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0 -- Best Fitness: 0.8497\n",
            "Generation 1 -- Best Fitness: 0.8732\n",
            "Generation 2 -- Best Fitness: 0.8811\n",
            "Generation 3 -- Best Fitness: 0.8836\n",
            "Generation 4 -- Best Fitness: 0.8926\n",
            "Generation 5 -- Best Fitness: 0.8924\n",
            "Generation 6 -- Best Fitness: 0.8957\n",
            "Generation 7 -- Best Fitness: 0.8988\n",
            "Generation 8 -- Best Fitness: 0.9014\n",
            "Generation 9 -- Best Fitness: 0.9054\n",
            "Accuracy of the pruned model: 0.8946\n",
            "Percentage of weights pruned: 49.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  approaches to achieve train BNNs\n",
        "#1- BinaryConnect or BinaryWeight Networks:\n",
        "# Use training techniques like BinaryConnect or BinaryWeight networks, where during the forward pass, weights are binarized to +1 or -1, and during the backward pass, real-valued weights are used for gradient descent.\n",
        "\n",
        "#2-Use Binary Neural Network Libraries:\n",
        "# Using specialized binary neural network libraries that provide tools and techniques for training binary networks. Examples include Brevitas, BNN-PYNQ\n",
        "\n",
        "#3-Quantization Techniques:\n",
        "# Apply quantization techniques to train a low-bit model. This is different from a strict binary model but allows weights to take on a limited set of values, such as -1, 0, and 1.\n",
        "\n",
        "#4-Post-Training Binarization:\n",
        "# Train a standard neural network first, and then apply post-training binarization to convert the weights to +1 or -1."
      ],
      "metadata": {
        "id": "NZmkUEfFqUu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9GMuqW5rNAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Post-Training Binarization Method\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "# Binarization function\n",
        "class BinaryActivation(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return input.sign()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output\n",
        "\n",
        "def binary_activation(input):\n",
        "    return BinaryActivation.apply(input)\n",
        "\n",
        "#Post-Training Binarization ()\n",
        "class BinaryNet_(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = binary_activation(self.fc1(x))\n",
        "        x = binary_activation(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n",
        "\n",
        "# Load MNIST\n",
        "mnist_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=mnist_transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=mnist_transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize and train the model\n",
        "model = BinaryNet()\n",
        "\n",
        "# Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Function to perform post-training binarization\n",
        "def binarize_weights(model):\n",
        "    binarized_model = copy.deepcopy(model)\n",
        "    for name, param in binarized_model.named_parameters():\n",
        "        param.data = param.data.sign()\n",
        "    return binarized_model\n",
        "\n",
        "# Apply post-training binarization\n",
        "binarized_model = binarize_weights(model)\n",
        "\n",
        "# Print all weights\n",
        "for name, param in binarized_model.named_parameters():\n",
        "    print(f'{name}: {param.data}')\n",
        "\n",
        "\n",
        "# Test the binarized model\n",
        "binarized_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = binarized_model(data)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy of the Post-Training Binarization Model: {accuracy * 100:.2f}%')\n",
        "torch.save(model.state_dict(), 'Post_Training_Binarization.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-PbqvIDqUm1",
        "outputId": "9e1cc9e1-f821-43ab-845a-a80154bf8330"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1.weight: tensor([[ 1., -1.,  1.,  ..., -1., -1., -1.],\n",
            "        [ 1.,  1., -1.,  ..., -1.,  1., -1.],\n",
            "        [ 1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
            "        ...,\n",
            "        [ 1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
            "        [-1.,  1., -1.,  ..., -1., -1.,  1.],\n",
            "        [ 1., -1.,  1.,  ..., -1., -1., -1.]])\n",
            "fc1.bias: tensor([-1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
            "         1., -1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
            "         1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "         1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
            "        -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,\n",
            "         1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "         1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
            "        -1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,\n",
            "        -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
            "        -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
            "        -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
            "         1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1.,\n",
            "         1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
            "         1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "         1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "         1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,\n",
            "        -1., -1., -1.,  1.,  1.,  1., -1., -1.])\n",
            "fc2.weight: tensor([[ 1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
            "        [-1.,  1., -1.,  ..., -1.,  1., -1.],\n",
            "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
            "        ...,\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [ 1., -1., -1.,  ..., -1., -1.,  1.],\n",
            "        [-1.,  1., -1.,  ...,  1., -1., -1.]])\n",
            "fc2.bias: tensor([-1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,\n",
            "         1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
            "         1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "        -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "         1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,\n",
            "         1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "         1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
            "         1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1.])\n",
            "fc3.weight: tensor([[-1.,  1., -1.,  ...,  1., -1., -1.],\n",
            "        [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
            "        [-1., -1., -1.,  ..., -1.,  1., -1.],\n",
            "        ...,\n",
            "        [-1., -1., -1.,  ..., -1.,  1.,  1.],\n",
            "        [-1.,  1., -1.,  ..., -1., -1.,  1.],\n",
            "        [ 1., -1.,  1.,  ...,  1.,  1., -1.]])\n",
            "fc3.bias: tensor([-1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.])\n",
            "Test Accuracy of the Post-Training Binarization Model: 90.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## prune Post-Training Binarization model\n",
        "def test_pruned_model(model, validation_dataset, best_pruned_mask):\n",
        "    # Apply the best mask to the model to get the pruned model\n",
        "    pruned_model = copy.deepcopy(model)\n",
        "    total_params = 0\n",
        "    pruned_params = 0\n",
        "\n",
        "    for name, param in pruned_model.named_parameters():\n",
        "        mask = best_pruned_mask.get(name, torch.ones_like(param.data).bool())\n",
        "        pruned_params += torch.sum(~mask).item()\n",
        "        total_params += torch.numel(param.data)\n",
        "\n",
        "        param.data.mul_(mask)\n",
        "\n",
        "    # Evaluate the pruned model on the validation dataset\n",
        "    accuracy = evaluate(pruned_model, validation_dataset)\n",
        "    print(f\"Accuracy of the pruned model: {accuracy}\")\n",
        "\n",
        "    # Calculate and print the percentage of weights pruned\n",
        "    percentage_pruned = (pruned_params / total_params) * 100\n",
        "    print(f\"Percentage of weights pruned: {percentage_pruned:.2f}%\")\n",
        "\n",
        "\n",
        "def evaluate(model, dataset):\n",
        "    correct, total = 0, 0\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    model = BinaryNet()\n",
        "    model.load_state_dict(torch.load('Post_Training_Binarization.pth'))\n",
        "\n",
        "\n",
        "    validation_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "    ga = PruningGeneticAlgorithm(model=model, dataset=validation_dataset)\n",
        "    best_pruned_mask = ga.run()\n",
        "\n",
        "    # Test the pruned model\n",
        "    test_pruned_model(model, validation_dataset, best_pruned_mask)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOGMX-oTw1Uv",
        "outputId": "74dafb38-0278-4536-ff4d-edbd2715983b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0 -- Best Fitness: 0.8625\n",
            "Generation 1 -- Best Fitness: 0.8792\n",
            "Generation 2 -- Best Fitness: 0.8902\n",
            "Generation 3 -- Best Fitness: 0.8932\n",
            "Generation 4 -- Best Fitness: 0.8979\n",
            "Generation 5 -- Best Fitness: 0.9022\n",
            "Generation 6 -- Best Fitness: 0.9035\n",
            "Generation 7 -- Best Fitness: 0.9054\n",
            "Generation 8 -- Best Fitness: 0.9081\n",
            "Generation 9 -- Best Fitness: 0.9078\n",
            "Accuracy of the pruned model: 0.9003\n",
            "Percentage of weights pruned: 50.01%\n"
          ]
        }
      ]
    }
  ]
}